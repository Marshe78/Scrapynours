{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWUK25s9YosK"
   },
   "source": [
    "# 4. Web Scraping - Scrapy\n",
    "\n",
    "<img src='https://xn--kvin-duranty-beb.fr/wp-content/uploads/2022/10/Web-Scraping-_-IPSSI-PRS-3.png'>\n",
    "\n",
    "Dans cet exercice, nous utiliserons la bibliothèque scrapy afin de collecter les données des sites internet suivants :\n",
    "\n",
    "- Partie 1 : [AlloCiné](https://www.allocine.fr/film/meilleurs)\n",
    "Nous collecterons les informations des meilleurs films recensés par la platforme.\n",
    "\n",
    "\n",
    "- Partie 2 : [Boursorama](https://www.boursorama.com/bourse/actions/palmares/france/page-1?france_filter%5Bmarket%5D=1rPCAC)\n",
    "Nous collecterons les données boursières des entreprises du CAC40.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5kwWF7QiXLH"
   },
   "source": [
    "# Parie 1 - Les meilleurs films selon [Allociné.fr](https://www.allocine.fr/film/meilleurs/)\n",
    "\n",
    "<img src= 'https://fr.web.img2.acsta.net/newsv7/15/10/19/21/14/237930.jpg'>\n",
    "\n",
    "L'objectif de cet exercice est de collecter les données des meilleurs films présents sur la page `https://www.allocine.fr/film/meilleurs/` du site allocine.fr.\n",
    "\n",
    "Les données que nous collecterons seront les suivantes :\n",
    "- Les titres du film\n",
    "- Les liens des images\n",
    "- Les noms des auteurs\n",
    "- Les durées des films\n",
    "- Les genres cinématographiques \n",
    "- Les scores des films\n",
    "- Les descriptions des films\n",
    "- Les dates de sortie des films\n",
    "\n",
    "## 4.1 Installez scrapy à l'aide de la commande suivante :\n",
    "\n",
    "\n",
    "`pip install scrapy`\n",
    "\n",
    "\n",
    "## 4.2 Générez un environement de travail scrapy en executant la commande suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapyNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script automat-visualize.exe is installed in 'c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts cftp.exe, ckeygen.exe, conch.exe, mailmail.exe, pyhtmlizer.exe, tkconch.exe, trial.exe, twist.exe and twistd.exe are installed in 'c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tldextract.exe is installed in 'c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script scrapy.exe is installed in 'c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading Scrapy-2.7.0-py2.py3-none-any.whl (270 kB)\n",
      "     -------------------------------------- 271.0/271.0 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting cryptography>=3.3\n",
      "  Downloading cryptography-38.0.1-cp36-abi3-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\33688\\appdata\\roaming\\python\\python310\\site-packages (from scrapy) (21.3)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting lxml>=4.3.0\n",
      "  Downloading lxml-4.9.1-cp310-cp310-win_amd64.whl (3.6 MB)\n",
      "     ---------------------------------------- 3.6/3.6 MB 4.7 MB/s eta 0:00:00\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-2.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting service-identity>=18.1.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\33688\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (63.2.0)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting Twisted>=18.9.0\n",
      "  Downloading Twisted-22.8.0-py3-none-any.whl (3.1 MB)\n",
      "     ---------------------------------------- 3.1/3.1 MB 4.8 MB/s eta 0:00:00\n",
      "Collecting pyOpenSSL>=21.0.0\n",
      "  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.0/57.0 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.9/93.9 kB 5.2 MB/s eta 0:00:00\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n",
      "Collecting zope.interface>=5.1.0\n",
      "  Downloading zope.interface-5.5.0-cp310-cp310-win_amd64.whl (211 kB)\n",
      "     -------------------------------------- 211.6/211.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.15.1-cp310-cp310-win_amd64.whl (179 kB)\n",
      "     -------------------------------------- 179.1/179.1 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\33688\\appdata\\roaming\\python\\python310\\site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Collecting attrs>=19.1.0\n",
      "  Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.8/58.8 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "     ---------------------------------------- 74.6/74.6 kB ? eta 0:00:00\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting typing-extensions>=3.6.5\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2\n",
      "  Downloading twisted_iocpsupport-1.0.2-cp310-cp310-win_amd64.whl (45 kB)\n",
      "     ---------------------------------------- 45.7/45.7 kB ? eta 0:00:00\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\33688\\appdata\\roaming\\python\\python310\\site-packages (from packaging->scrapy) (3.0.9)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Collecting requests>=2.1.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting idna\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.5/61.5 kB ? eta 0:00:00\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "     -------------------------------------- 118.7/118.7 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.4/140.4 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
      "     -------------------------------------- 161.1/161.1 kB 3.2 MB/s eta 0:00:00\n",
      "Using legacy 'setup.py install' for PyDispatcher, since package 'wheel' is not installed.\n",
      "Installing collected packages: twisted-iocpsupport, PyDispatcher, pyasn1, incremental, constantly, zope.interface, w3lib, urllib3, typing-extensions, queuelib, pycparser, pyasn1-modules, protego, lxml, jmespath, itemadapter, idna, filelock, cssselect, charset-normalizer, certifi, attrs, requests, parsel, hyperlink, cffi, Automat, Twisted, requests-file, itemloaders, cryptography, tldextract, service-identity, pyOpenSSL, scrapy\n",
      "  Running setup.py install for PyDispatcher: started\n",
      "  Running setup.py install for PyDispatcher: finished with status 'done'\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.6 Twisted-22.8.0 attrs-22.1.0 certifi-2022.9.24 cffi-1.15.1 charset-normalizer-2.1.1 constantly-15.1.0 cryptography-38.0.1 cssselect-1.1.0 filelock-3.8.0 hyperlink-21.0.0 idna-3.4 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 lxml-4.9.1 parsel-1.6.0 protego-0.2.1 pyOpenSSL-22.1.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 queuelib-1.6.2 requests-2.28.1 requests-file-1.5.1 scrapy-2.7.0 service-identity-21.1.0 tldextract-3.4.0 twisted-iocpsupport-1.0.2 typing-extensions-4.4.0 urllib3-1.26.12 w3lib-2.0.1 zope.interface-5.5.0\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SSXagNDniXLI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'WebCrawler', using template directory 'C:\\Users\\33688\\anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    C:\\Users\\33688\\Documents\\python\\scrapping\\OneDrive_2_26-10-2022\\WebCrawler\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd WebCrawler\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "# Création du dossier WebCrawler contenant l'ensemble des fichiers utiles au fonctionnement de scrapy\n",
    "!scrapy startproject WebCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dvKu-RjMiXLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'allocine' using template 'basic' in module:\n",
      "  WebCrawler.spiders.allocine\n"
     ]
    }
   ],
   "source": [
    "# Création du projet AlloCiné dans le dossier WebCrawler/spider\n",
    "!cd WebCrawler && scrapy genspider allocine https://www.allocine.fr/film/meilleurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyGgGtBPiXLK"
   },
   "source": [
    "Les fichiers de scrapy que nous manipulerons seront :\n",
    "\n",
    "1. le fichier `items.py` qui contient les champs que nous souhaitons collecter (ex : nom des films, score, date de publication). Chaque champs sera introduit dans la class `ReviewsAllocineItem` avec la nomenclature suivante : `name = scrapy.Field()`.\n",
    "\n",
    "\n",
    "2. le fichier `allocine.py` qui contient les fonctions qui permetons la collecte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B58lLGGkiXLK"
   },
   "source": [
    "## 4.3 Ajoutez dans le fichier `items.py` les champs que nous souhaitons collecter.\n",
    "\n",
    "Ajoutez dans la class `ReviewsAllocineItem(scrapy.Item)` les champs suivants,\n",
    "pour rappel la nomenclature des champs est la suivante : \n",
    "\n",
    "`name = scrapy.Field()`.\n",
    "\n",
    "- title\n",
    "- img\n",
    "- author\n",
    "- time\n",
    "- genre\n",
    "- score\n",
    "- desc\n",
    "- release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m59XddkMiXLL"
   },
   "outputs": [],
   "source": [
    "#À ajouter au fichier items.py\n",
    "class ReviewsAllocineItem(scrapy.Item):\n",
    "     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-oa-axJiXLM"
   },
   "source": [
    "## 4.3 Lancez votre terminal puis exécutez la commande suivante :\n",
    "\n",
    "C'est dans le terminale que nous intéragirons avec scrapy pour manipuler les bases du site allocine.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpm98jI_iXLM"
   },
   "source": [
    "`scrapy shell`\n",
    "\n",
    "`url = 'https://www.allocine.fr/film/meilleurs'`\n",
    "\n",
    "`fetch(url)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6Y4IOoriXLN"
   },
   "source": [
    "## 4.4 Exécutez dans votre terminal les commandes suivantes et notez vos observations.\n",
    "\n",
    "Repérez en parallèle à quelles balises correspondent vos résultats.\n",
    "\n",
    "`response`\n",
    "<200 https://www.allocine.fr/film/meilleurs/>\n",
    "\n",
    "`response.css('a')`\n",
    "[<Selector xpath='descendant-or-self::a' data='<a href=\"/tag-1026/films/\">Meilleurs ...'>,\n",
    " <Selector xpath='descendant-or-self::a' data='<a href=\"/tag-678/films/\">Meilleurs f...'>,\n",
    " <Selector xpath='descendant-or-self::a' data='<a class=\"item\" href=\"/\">Accueil</a>'>,\n",
    " <Selector xpath='descendant-or-self::a' data='<a class=\"item\" href=\"/film/\">CinÚma</a>'>,\n",
    " <Selector xpath='descendant-or-self::a' data='<a class=\"item\" href=\"/film/aucinema/...'>,\n",
    " <Selector xpath='descendant-or-self::a' data='<a class=\"item\" href=\"/film/meilleurs...'>,\n",
    " <Selector xpath='descendant-or-self::a' data='<a class=\"item js-item-mq-medium\" hre...'>,\n",
    " <Selector xpath='descendant-or-self::a' data='<a class=\"item js-item-mq-medium\" hre...'>,\n",
    " <Selector xpath='descendant-or-self::a' data='<a class=\"item js-item-mq-medium\" hre...'>,\n",
    " <Selector xpath='descendant-or-self::a' data='<a class=\"item-content\" href=\"/film/m...'>,\n",
    "\n",
    "`len(response.css('a'))`\n",
    "126\n",
    "\n",
    "`response.css('a::text')`\n",
    "<Selector xpath='descendant-or-self::a/text()' data='Meilleurs teen movies'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Meilleurs films de zombies'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Accueil'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='CinÚma'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data=\"Meilleurs films Ó l'affiche\">,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Meilleurs films selon la presse'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Meilleurs documentaires'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Meilleurs films au box-office'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Les pires films'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Action'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Animation'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Aventure'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Biopic'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='ComÚdie'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='ComÚdie dramatique'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Drame'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Epouvante-horreur'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Famille'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Fantastique'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Guerre'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Historique'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Musical'>,\n",
    " <Selector xpath='descendant-or-self::a/text()' data='Policier'>,\n",
    "\n",
    "`response.css('a')[0].attrib`\n",
    "{'href': '/tag-1026/films/'}\n",
    "\n",
    "`response.css('a')[0].attrib['href']`\n",
    "'/tag-1026/films/'\n",
    "\n",
    "`response.css('a::text')[0].extract()`\n",
    "'Meilleurs teen movies'\n",
    "\n",
    "`response.css('h1.item')`\n",
    "<Selector xpath=\"descendant-or-self::h1[@class and contains(concat(' ', normalize-space(@class), ' '), ' item ')]\" data='<h1 class=\"item\">\\nTop films </h1>'>\n",
    "\n",
    "\n",
    "`response.css('h1.item::text')`\n",
    "<Selector xpath=\"descendant-or-self::h1[@class and contains(concat(' ', normalize-space(@class), ' '), ' item ')]/text()\" data='\\nTop films '>\n",
    "\n",
    "`response.css('h1.item::text')[0].extract()`\n",
    "'\\nTop films '\n",
    "\n",
    "`response.css('img')`\n",
    "[<Selector xpath='descendant-or-self::img' data='<img class=\"header-main-logo-img\" src...'>,\n",
    " <Selector xpath='descendant-or-self::img' data='<img class=\"avatar js-myaccount-avata...'>,\n",
    " <Selector xpath='descendant-or-self::img' data='<img class=\"avatar js-myaccount-avata...'>,\n",
    " <Selector xpath='descendant-or-self::img' data='<img class=\"thumbnail-img\" src=\"https...'>,\n",
    " <Selector xpath='descendant-or-self::img' data='<img class=\"thumbnail-img\" src=\"https...'>,\n",
    " <Selector xpath='descendant-or-self::img' data='<img class=\"thumbnail-img\" src=\"data:...'>,\n",
    " <Selector xpath='descendant-or-self::img' data='<img class=\"thumbnail-img\" src=\"data:...'>,\n",
    " <Selector xpath='descendant-or-self::img' data='<img class=\"thumbnail-img\" src=\"data:...'>,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w6RdofbiXLN"
   },
   "source": [
    "## 4.5 Recherchez les informations suivantes sous forme de chaine de caractère (str) :\n",
    "\n",
    "0. La balise qui liste l'ensemble des films\n",
    "1. Le titre du premier film\n",
    "2. Le lien de l'image du premier film\n",
    "3. Le nom de l'auteur du premier film\n",
    "4. La durée du premier film \n",
    "5. Le genre cinématographique du premier film\n",
    "6. Le score du premier film\n",
    "7. La description du premier film\n",
    "8. La date de sortie du premier film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ya8Hcm3XiXLN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 0. La balise qui liste l'ensemble des films\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response\u001b[39m.\u001b[39mcss(\u001b[39m'\u001b[39m\u001b[39mentity-card-list\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "# 0. La balise qui liste l'ensemble des films\n",
    "response.css('.meta-title-link::text').extract()\n",
    "\n",
    "#1. Le titre du premier film\n",
    "response.css('.meta-title-link::text')[0].extract()\n",
    "\n",
    "\n",
    "#2. Le lien de l'image du premier film\n",
    "response.css('img.thumbnail-img')[0].attrib['src']\n",
    "\n",
    "#3. Le nom de l'auteur du premier film\n",
    "response.css('.blue-link::text')[0].extract()\n",
    "\n",
    "#4. La durée du premier film \n",
    "response.css('.meta-body-info::text')[0].extract()\n",
    "\n",
    "#5. Le genre cinématographique du premier film\n",
    "response.css('.meta-body-item.meta-body-info')[0].css('span::text').extract()\n",
    "\n",
    "#6. Le score du premier film\n",
    "response.css('.stareval-note')[0].extract()\n",
    "\n",
    "#7. La description du premier film\n",
    "response.css('.content-txt::text')[0].extract()\n",
    "\n",
    "#8. La date de sortie du premier film\n",
    " response.css('.meta-body-item').css('.date::text')[0].extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOl4emv1iXLO"
   },
   "source": [
    "## 4.6 Complétez le code suivante en fonction des résultats obtenus à la question précédentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3baQj3V4iXLO"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Request\n",
    "from items import WebcrawlerItem\n",
    "\n",
    "\n",
    "class AllocineSpider(scrapy.Spider):\n",
    "    name = 'allocine'\n",
    "    allowed_domains = ['www.allocine.fr']\n",
    "    \n",
    "    #Liste des pages à collecter\n",
    "    start_urls = [f'https://www.allocine.fr/film/meilleurs/?page={n}' for n in range(1,10)]\n",
    "\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield Request(url=url, callback=self.parse_manga)\n",
    "        \n",
    "        \n",
    "    def parse_manga(self, response):\n",
    "        liste_film = response.css('.meta-title-link::text').extract()\n",
    "        \n",
    "        \n",
    "        # Boucle qui parcours l'ensemble des éléments de la liste des films\n",
    "        for film in liste_film:\n",
    "            item = ReviewsAllocineItem()\n",
    "\n",
    "            # Nom du film\n",
    "            try:\n",
    "                item['title'] = response.css('.meta-title-link::text')[0].extract()\n",
    "            except:\n",
    "                item['title'] = 'None'\n",
    "              \n",
    "            # Lien de l'image du film\n",
    "            try:\n",
    "                item['img'] = response.css('img.thumbnail-img')[0].attrib['src']\n",
    "            except:\n",
    "                item['img'] = 'None'\n",
    "\n",
    "\n",
    "            # Auteur du film\n",
    "            try:\n",
    "                item['author'] = response.css('.blue-link::text')[0].extract()\n",
    "            except:\n",
    "                item['author'] = 'None'\n",
    "           \n",
    "            # Durée du film\n",
    "            try:\n",
    "                item['time'] = response.css('.meta-body-info::text')[0].extract()\n",
    "            except:\n",
    "                item['time'] = 'None'\n",
    "\n",
    "            # Genre cinématographique\n",
    "            try:\n",
    "                item['genre'] = response.css('.meta-body-item.meta-body-info')[0].css('span::text').extract()\n",
    "            except:\n",
    "                 item['genre'] = 'None'\n",
    "\n",
    "            # Score du film\n",
    "            try:\n",
    "                item['score'] = response.css('.stareval-note')[0].extract()\n",
    "            except:\n",
    "                item['score'] = 'None'\n",
    "\n",
    "            # Description du film\n",
    "            try:\n",
    "                item['desc'] = response.css('.content-txt::text')[0].extract()\n",
    "            except:\n",
    "                item['desc'] = 'None'\n",
    "\n",
    "            # Date de sortie\n",
    "            try:\n",
    "                item['release'] = response.css('.meta-body-item').css('.date::text')[0].extract()\n",
    "            except:\n",
    "                item['release'] = 'None'\n",
    "\n",
    "            yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5tuGKHsiXLO"
   },
   "source": [
    "## 4.7 Ajoutez l'ensemble de votre code au fichier `allocine.py` se trouvant dans le dossier `spider`, exécutant la commande suivante afin d'obtenir le fichier `allocine.csv` contenant les données collectées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tzo8p7naiXLO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in execute\n",
      "    cmd.crawler_process = CrawlerProcess(settings)\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\crawler.py\", line 304, in __init__\n",
      "    super().__init__(settings)\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\crawler.py\", line 181, in __init__\n",
      "    self.spider_loader = self._get_spider_loader(settings)\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\crawler.py\", line 175, in _get_spider_loader\n",
      "    return loader_cls.from_settings(settings.frozencopy())\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\spiderloader.py\", line 67, in from_settings\n",
      "    return cls(settings)\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\spiderloader.py\", line 24, in __init__\n",
      "    self._load_all_spiders()\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\spiderloader.py\", line 51, in _load_all_spiders\n",
      "    for module in walk_modules(name):\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 88, in walk_modules\n",
      "    submod = import_module(fullpath)\n",
      "  File \"c:\\Users\\33688\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\33688\\Documents\\python\\scrapping\\OneDrive_2_26-10-2022\\WebCrawler\\WebCrawler\\spiders\\allocine.py\", line 3, in <module>\n",
      "    from items import ReviewsAllocineItem\n",
      "ModuleNotFoundError: No module named 'items'\n"
     ]
    }
   ],
   "source": [
    "!cd spiders && scrapy crawl allocine -o allocine2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fFbZxGAiXLP"
   },
   "source": [
    "## 4.7 Importez la bibliothèque Pandas puis visualisez votre collecte de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4z-ZPA-BiXLP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWwF7o_DiXLP"
   },
   "source": [
    "# Partie 2 - Cotations boursières du CAC40 - [Boursorama](https://www.boursorama.com/bourse/actions/palmares/france/page-1?france_filter%5Bmarket%5D=1rPCAC)\n",
    "\n",
    "\n",
    "<img src='data:image/png;base64,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'>\n",
    "\n",
    "\n",
    "\n",
    "L'objectif de cet exercice sera de collecter les données en temps réelle des actions du CAC40.\n",
    "Les données que nous collecterons serons :\n",
    "\n",
    "- le nom de l'indice boursier\n",
    "- le cours de l'action\n",
    "- la variation de l'action\n",
    "- la valeur la plus haute de la séance\n",
    "- la valeur la plus basse\n",
    "- la valeur d'ouverture\n",
    "- la date et l'heure de la collecte\n",
    "\n",
    "\n",
    "## Lancez le projet scrapy\n",
    "\n",
    "Créez le projet scrapy nomé `boursorama` en utilisant le lien suivant `https://www.boursorama.com/bourse/actions/palmares/france/page-1`.\n",
    "\n",
    "Pour rappel la commande scrapy pour génére un nouveau spider est la suivante : \n",
    "\n",
    "`scrapy genspider nom_du_projet adresse_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rr04zt80iXLQ"
   },
   "outputs": [],
   "source": [
    "!cd WebCrawler && scrapy genspider (nom_du_projet::à compléter) (url_du_projet::à compléter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yrj1tdZviXLQ"
   },
   "source": [
    "## Modifiez le fichier items.py en ajoutant les champs à collecter\n",
    "\n",
    "Ajoutez une class `ReviewsBoursoramaItem(scrapy.Item)` puis les champs avec la nomenclature `name = scrapy.Field()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4CdiIVwiXLQ"
   },
   "source": [
    "## Lancez le shell scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUDEYKAJiXLQ"
   },
   "source": [
    "`scrapy shell`\n",
    "\n",
    "`url = 'https://www.boursorama.com/bourse/actions/palmares/france/page-1?france_filter%5Bmarket%5D=1rPCAC'`\n",
    "\n",
    "`fetch(url)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHOTe-TMiXLQ"
   },
   "source": [
    "## Compléter le code suivant puis ajoutez-le au fichier `boursorama.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQQpUbnUiXLQ"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Request\n",
    "from WebCrawler.items import #Importe la class contenant vos items (champs collectés) ==> à compléter\n",
    "import time\n",
    "\n",
    "class BoursoramaSpider(scrapy.Spider):\n",
    "    name = 'boursorama'\n",
    "    allowed_domains = ['finance.yahoo.com']\n",
    "    start_urls = #[Liste des URL à compléter]\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield Request(url=url, callback=self.parse_boursorama)\n",
    "            \n",
    "    def parse_boursorama(self, response):\n",
    "        liste_indices = response.css('tr.c-table__row')[1:]\n",
    "        \n",
    "        for indices in liste_indices:\n",
    "            item = #importer la class Items du projet provenant du fichier items.py\n",
    "            \n",
    "            #indice boursier\n",
    "            try: \n",
    "              item['indice'] = #à compléter\n",
    "            except:\n",
    "              item['indice'] = 'None'\n",
    "            \n",
    "            #indice cours de l'action\n",
    "            try: \n",
    "              item['cours'] = #à compléter\n",
    "            except:item['cours'] = 'None'\n",
    "            \n",
    "            #Variation de l'action\n",
    "            try: \n",
    "              item['var'] = #à compléter\n",
    "            except:\n",
    "              item['var'] = 'None'\n",
    "            \n",
    "            #Valeur la plus haute\n",
    "            try: \n",
    "              item['hight'] = #à compléter\n",
    "            except:\n",
    "              item['hight'] = 'None'\n",
    "            \n",
    "            #Valeur la plus basse\n",
    "            try: \n",
    "              item['low'] = #à compléter\n",
    "            except:\n",
    "              item['low'] = 'None'\n",
    "\n",
    "            #Valeur d'ouverture\n",
    "            try: \n",
    "              item['open_'] = #à compléter\n",
    "            except:\n",
    "              item['open_'] = 'None'\n",
    "\n",
    "            #Date de la collecte\n",
    "            try: \n",
    "              item['time'] = #à compléter\n",
    "            except:\n",
    "              item['time'] = 'None'\n",
    "\n",
    "            \n",
    "            yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CW6by-UiXLQ"
   },
   "source": [
    "## Executez la commande suivante afin de collecter vos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xaWKWlViXLR"
   },
   "outputs": [],
   "source": [
    "!cd WebCrawler/WebCrawler/spiders && scrapy crawl (nom_du_projet::à compléter) -o (nom_du_fichier_csv::à compléter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cDVct8giXLR"
   },
   "source": [
    "# Bonus - [MyAnimeListe](https://myanimelist.net/manga.php?letter=A)\n",
    "\n",
    "<img src='https://image.myanimelist.net/ui/OK6W_koKDTOqqqLDbIoPArR89MP-ulHxaLCJ2P-BfXg'>\n",
    "\n",
    "Reproduisez l'ensemble de la procédure en collectant les données du site [MyAnimeListe](https://myanimelist.net/manga.php?letter=A).\n",
    "\n",
    "Les données à collecter : \n",
    "- le nom des animés\n",
    "- l'image des animés\n",
    "- la description des animés.\n",
    "\n",
    "Utilisez la class `DataBase` pour stoquer vos donneés dans une base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuCtNZi6iXLR"
   },
   "outputs": [],
   "source": [
    "import sqlalchemy as db\n",
    "\n",
    "class DataBase():\n",
    "    def __init__(self, name_database='database'):\n",
    "        self.name = name_database\n",
    "        self.url = f\"sqlite:///{name_database}.db\"\n",
    "        self.engine = db.create_engine(self.url)\n",
    "        self.connection = self.engine.connect()\n",
    "        self.metadata = db.MetaData()\n",
    "        self.table = self.engine.table_names()\n",
    "       \n",
    "    \n",
    "    def create_table(self, name_table, **kwargs):\n",
    "        colums = [db.Column(k, v, primary_key = True) if 'id_' in k else db.Column(k, v) for k,v in kwargs.items()]\n",
    "        db.Table(name_table, self.metadata, *colums)\n",
    "        self.metadata.create_all(self.engine)\n",
    "        print(f\"Table : '{name_table}' are created succesfully\")\n",
    "        \n",
    "    def read_table(self, name_table, return_keys=False):\n",
    "        table = db.Table(name_table, self.metadata, autoload=True, autoload_with=self.engine)\n",
    "        if return_keys:table.columns.keys() \n",
    "        else : return table\n",
    "        \n",
    "        \n",
    "    def add_row(self, name_table, **kwarrgs):\n",
    "        name_table = self.read_table(name_table)\n",
    "        \n",
    "        stmt = (\n",
    "            db.insert(name_table).\n",
    "            values(kwarrgs)\n",
    "        )\n",
    "        self.connection.execute(stmt)\n",
    "        print(f'Row id added')\n",
    "        \n",
    "        \n",
    "    def delete_row_by_id(self, table, id_):\n",
    "        name_table = self.read_table(name_table) \n",
    "        \n",
    "        stmt = (\n",
    "            db.delete(name_table).\n",
    "            where(students.c.id_ == id_)\n",
    "            )\n",
    "        self.connection.execute(stmt)\n",
    "        print(f'Row id {id_} deleted')\n",
    "        \n",
    "    def select_table(self, name_table):\n",
    "        name_table = self.read_table(name_table)       \n",
    "        stm = db.select([name_table])\n",
    "        return self.connection.execute(stm).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNdFB96TiXLR"
   },
   "outputs": [],
   "source": [
    "#Générer un projet scrapy\n",
    "!cd WebCrawler && scrapy genspider manga https://myanimelist.net/manga.php?letter=A\n",
    "    \n",
    "# Collecter les données\n",
    "!cd WebCrawler/WebCrawler/spiders && scrapy crawl (nom_du_projet::à compléter) -o (nom_du_fichier_CSV::à compléter)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ecdb7dbd4c4fb660ee30794d8df8dc04ef05a319035caebffdab12c270025341"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
